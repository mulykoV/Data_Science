{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['The quick brown fox jumps over the lazy dogs.', 'Running, jumped, and better are words to test.']\n",
      "Word Tokenization: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs', '.', 'Running', ',', 'jumped', ',', 'and', 'better', 'are', 'words', 'to', 'test', '.']\n",
      "Stemming: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'run', ',', 'jump', ',', 'and', 'better', 'are', 'word', 'to', 'test', '.']\n",
      "Lemmatization (NLTK): ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Running', ',', 'jumped', ',', 'and', 'better', 'are', 'word', 'to', 'test', '.']\n",
      "Bag of Words:\n",
      " [[0 0 0 1 1 1 0 0 1 1 1 1 2 0]\n",
      " [1 0 0 1 1 0 0 1 0 0 0 1 1 0]\n",
      " [0 1 1 0 0 0 1 0 0 0 0 0 0 1]]\n",
      "Feature Names: ['and' 'animals' 'are' 'brown' 'dog' 'fox' 'foxes' 'is' 'jumps' 'lazy'\n",
      " 'over' 'quick' 'the' 'wild']\n",
      "\n",
      "TF-IDF:\n",
      " [[0.         0.         0.         0.26807016 0.26807016 0.35248004\n",
      "  0.         0.         0.35248004 0.35248004 0.35248004 0.26807016\n",
      "  0.53614032 0.        ]\n",
      " [0.48148213 0.         0.         0.36617957 0.36617957 0.\n",
      "  0.         0.48148213 0.         0.         0.         0.36617957\n",
      "  0.36617957 0.        ]\n",
      " [0.         0.5        0.5        0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.5       ]]\n",
      "Feature Names: ['and' 'animals' 'are' 'brown' 'dog' 'fox' 'foxes' 'is' 'jumps' 'lazy'\n",
      " 'over' 'quick' 'the' 'wild']\n",
      "\n",
      "Word Embedding (Word2Vec) for 'fox': [ 0.07898068 -0.06989504 -0.09155865 -0.00355753 -0.03099841  0.07894317\n",
      "  0.05938574 -0.01545663  0.01510963  0.01790041]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Завантажуємо англійську модель Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Тестовий текст\n",
    "text = \"The quick brown fox jumps over the lazy dogs. Running, jumped, and better are words to test.\"\n",
    "\n",
    "# === Токенізація ===\n",
    "# Токенізація речень\n",
    "sent_tokens = [sent.text for sent in nlp(text).sents]\n",
    "print(\"Sentence Tokenization:\", sent_tokens)\n",
    "\n",
    "# Токенізація слів\n",
    "word_tokens = [token.text for token in nlp(text)]\n",
    "print(\"Word Tokenization:\", word_tokens)\n",
    "\n",
    "# === Стемінг ===\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in word_tokens]\n",
    "print(\"Stemming:\", stems)\n",
    "\n",
    "# === Лемматизація ===\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print(\"Lemmatization (NLTK):\", lemmas)\n",
    "\n",
    "# Тестові дані\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog is quick and brown.\",\n",
    "    \"Foxes are wild animals.\"\n",
    "]\n",
    "\n",
    "# === Bag of Words ===\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow = bow_vectorizer.fit_transform(documents)\n",
    "print(\"Bag of Words:\\n\", bow.toarray())\n",
    "print(\"Feature Names:\", bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# === TF-IDF ===\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "print(\"\\nTF-IDF:\\n\", tfidf.toarray())\n",
    "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# === Word Embeddings (Word2Vec) ===\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_docs, vector_size=10, window=2, min_count=1)\n",
    "print(\"\\nWord Embedding (Word2Vec) for 'fox':\", word2vec_model.wv['fox'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
